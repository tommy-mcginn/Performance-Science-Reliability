---
title: "Reliability and Individual Change"
author: "Tommy McGinn"
date: "7/13/20"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reliability and Individual Change}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

#Making Sense of Measures of Reliability

##Intraclass Correlation Coefficient

The Intraclass Correlation Coefficient (ICC) is a unitless measure of how reliable a measurement instrument is, on a scale from 0 to 1. Higher values (i.e. values closer to 1) represent more reliable instruments. The ICC function in this package is simply a wrapper function of the ICC function found in the psych package with key advantages (see the related vignette for more details on the specific differences). The ICC output for each metric entered by the user as a function argument shows the values and related statistics for six different types of ICC's.

##Typical Error/Standard Error of Measurement

The Typical Error (TE) is essentially the within-subject standard deviation. It is a measure of how much one should "typically" expect the measurements of each athlete to vary from trial to trial, due to noise in the data. Such noise could be the result of biological variations among the athletes and/or measurement error on the part of the instrument being used to record the results of the test that the athletes are completing. If there are just two trials being analyzed, the TE is just the standard deviation of the difference scores of each athlete between the first trial and the second trial divided by root 2. 

If more than two trials are being studied, the TE is the root mean squared error (RMSE), or residual standard error, of a linear model of the subject and trial vectors (as factor variables) predicting the scores observed in the data. This is executed for each metric with the following code:

```{r, eval = FALSE}
lm_TE <- lm(metric ~ subject + trial)
TE = sigma(lm_TE)
```

The above code computes the correct TE value no matter how many trials there are (including two), so it is the method used in the TE function regardless of the number of trials involved in each vector of athlete measurements.

The Standard Error of Measurement (SEM) is another way of quantifying the reliability or measurement error of a particular instrument, and should in theory yield the same value as the TE. In practice, however, the TE might be slightly different than the SEM, as the SEM depends on the choice of reliability measurement for a given metric, while the TE does not. The formula for the SEM is the between-subject standard deviation of each athlete's scores multiplied by root (1 - reliability), with reliability being the measure of reliability from 0 to 1 (i.e. the ICC). 

##Coefficient of Variation

The Coefficient of Variation (CV) is similar to the TE, but it is expressed as a percentage of the mean value of the measurement. First, a CV is calculated for each athlete as the standard deviation of that athlete's measurements divided by the mean of that athlete's measurements. Then, the mean of all of these individual-level CV's is computed as the group-level CV of the metric of interest. The advantage of using this measure as opposed to the TE is that the CV is a unitless measure, which allows for easy comparison of metrics with different units and scales of measurement, whereas the TE is entirely dependent on unit and scale.

#Making Sense of Measures of Individual Change

##Smallest Worthwhile Change

The Smallest Worthwhile Change (SWC) is seen as the smallest change an athlete could achieve that we could confidently identify as a legitimate change, and not just the result of random or measurement error. The SWC is simply one-fifth of the between-athlete standard deviation. In order for a measurement to be considered reliable, the SWC should be larger than the TE. If this is not the case, then a change amounting to the SWC could just be the result of measurement error and not a real change. 

For the SWC function (and the SEM and MDC functions), the user can choose which method should be used for the computation of the between-athlete standard deviation involved in the formula. Does the user want to average each subject's values and use the standard deviation of these averages (the default), or should only each subject's maximum or minimum value be considered? This is the purpose of the group_by and summarise functions from the dplyr package that are used inside of the SWC, SEM, and RCI functions:

```{r, eval = FALSE}
if (method == AVG) {

  df <- group_by(df, subject)
  df <- summarise(df, across(where(is.numeric), ~ mean(.x)))

} else if (method == MAX) {

  df <- group_by(df, subject)
  df <- summarise(df, across(where(is.numeric), ~ max(.x)))

} else {

  df <- group_by(df, subject)
  df <- summarise(df, across(where(is.numeric), ~ min(.x)))

}
```

##Minimal Detectable Change

With the same goal in mind as the SWC of capturing individual change, the Minimal Detectable Change (MDC) is simply the SEM (or alternatively, the TE) multiplied by root 2 and then multiplied by a critical value that corresponds to a confidence level that the user chooses. The confidence level is the level of certainty one can have that the MDC is a real change and not due to random or measurement error. The default confidence level in our function is 0.95, meaning that the MDC that comes out of our function for a given metric is the one that, if it were to ba achieved by an athlete in the data, one could 95% confident is a legitimate change in performance, based on the corresponding critical value from the standard normal distribution.

##Reliable Change Index

The Reliable Change Index (RCI) is an extension of the MDC and is specifically focused on comparing an athlete's scores at two distinct points in time. It uses the MDC to determine the threshold of what constitutes an actual change in performance, both positive and negative. To this end, our RCI function yields two vectors, one for the lower bound and another for the upper bound, that define the thresholds for identifying a change as reliable from each individual's measurement(s) at a specific point in time (explicitly identified by the user), such that if the individual were to score lower than his or her lower bound or higher than his or her upper bound, a reliable change would be said to have occurred.   
