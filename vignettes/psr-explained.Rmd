title: "psr Explained"
authors: "Alan Huebner", "Tommy McGinn", and "Matthew Sisk"
date: "03/08/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{psr Explained}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# 1. Motivation for Package

The psr package is intended for coaches and analysts in the field of sports performance science. It aims to allow users to efficiently calculate metrics for assess measurement reliability as well as individual change in athletic performance. All functions take as inputs vectors containing measurements taken on an athletes performing strength and agility tests. (WE NEED TO TALK ABOUT THIS WORDING WITH MATT) (MLS: Yeah, I think it needs a little, is performing strength the name of a test? Can it be capitalized?)

psr can be used to assess measurement reliability via functions that compute the typical error (TE), coefficient of variation (CV), standard error of measurement (SEM), and intraclass correlation coefficient (ICC_long) functions. ICC_long() builds on psych::ICC() by taking data in long format as its input, saving the user the task of pivoting the data to wide format. Furthermore, ICC_long allows for the computation of ICC's for multiple metrics at once.

The smallest worthwhile change (SWC) and minimal detectable change (MDC) functions assess individual change of athletes, but are still group-wide metrics. An Individual-level metric also appears in the package, in the form of the standard ten score (STEN) function. STEN() takes each athlete's scores for every metric and standardizes them to a 1-10 scale, allowing the user to easily see an athlete's strengths and weaknesses relative to his or her peers. This vignette illustrates the use and intreprets the output of each function via example data.

# 2. Reliability of Measurement Instrument

The psr package includes four different functions for assessing reliability of measurements: TE(), CV(), SEM(), and ICC_long(). Each will be discussed in turn.

## 2.1. Typical Error: TE()

The Typical Error (TE) is the within-athlete variation of the athlete measurements in the data (MLS: This seems like it could be simplified). It is a measure of how much one should "typically" expect the measurements of each athlete to vary from trial to trial, due to noise in the data. Such noise could be the result of biological variation in the athletes or measurement error on the part of the instrument being used to record the in the test that the athletes are completing. 

If there are just two trials being analyzed, the TE is just the standard deviation of the difference scores of each athlete between the first trial and the second trial, divided by root 2 (Hopkins, 2000).

If more than two trials are being studied, the TE is the residual standard error of a linear model with the metric as the response and the subject and trial as independent variable, entered into the model as factors (Hopkins, 2000). It represents the error involved in predicting the value of a metric based on the athlete ID and trial entered into the model as factors.  

The following data set consists of three different athletes performing three trials of a certain task, and three different metrics on biomechanical measurements are recorded:

```{r}
subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3')
metric_1 <- c(250, 258, 252, 279, 270, 277, 218, 213, 218)
metric_2 <- c(10, 7, 10, 14, 18, 17, 11, 7, 8)
metric_3 <- c(1214, 1276, 1289, 1037, 1010, 1069, 1481, 1465, 1443)
```

The TE() function is called by specifying the rows containing the subjects, trials, and metrics:

```{r}
psr::TE(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# metric_1 metric_2 metric_3
# 4.690416 2.309401 34.77946
```

TE is expressed in terms of the units of the original metric. Thus, the TE of metric_3 is much larger than the TE's of the other two metrics, but that is largely due to the fact that metric_3 is on a different scale than metric_1 and metric_2.

## 2.2. Coefficient of Variation: CV()

The Coefficient of Variation (CV) is the TE expressed as a percentage of the mean of the data. It is calculated by first computing the TE of each metric, and dividing this value by the mean of the metric, across all athletes and trials. There are inconsistent explanations of the CV in the performance science literature, and this package follows the interpretation found in Hopkins (2000), which denotes the CV as the typical percentage error. Thus, the TE is expressed in absolute terms, while the CV is expressed in percentage terms. The CV is a unitless measure, which allows for easy comparison of metrics with different units and scales of measurement, whereas the interpretation of the TE is entirely dependent on unit and scale.

```{r}
psr::CV(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# metric_1 metric_2 metric_3
# 1.888758 20.37707 2.773974
```

Using the same data as in the section above, it is seen that the CV of metric_2 is actually much larger than the CV's of the other two metrics. Hopkins notes that the CV for a test will typically be 1-5% (Hopkins, 2000). The CV's of metric_1 and metric_3, respectively, fall within this range, while the CV of metric_2 does not. This underscores the fact that metric_2 has the smallest TE of any of the three metrics largely because all of its measurements are much smaller in magnitude than the measurements of the other two metrics, meaning that its variation is lower in absolute terms than the variation of the other two metrics. As a percentage of its mean, however, metric_2 appears to be much less reliable than both metric_1 and metric_3.   

## 2.3. Standard Error of Measurement: SEM()

The Standard Error of Measurement (SEM) is similar to the TE and CV, and is a way of quantifying the amount of variability between athletes. It is the deviation around the true mean. Unlike the two previous measurements: SEM utilizes the ICC, which is another measure of reliability. The formula for the SEM is the between-subject standard deviation of each athlete's scores multiplied by root (1 - ICC), as described in Atkinson & Nevill (1998).

![Alt text] (/Users/tmcginn/Desktop/Unknown-1.png)

Running the code for SEM(), using the same data as in the case of the TE() and CV():

```{r}
psr::SEM(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.99, 0.93, 0.99))
```

As noted earlier, SEM() adds one additional argument to those in TE() and CV(). This addition is a list of the individual ICC values for the metrics, which must be entered in order. In the above example, the ICC values for metric_1, metric_2, and metric_3 are 0.92, 0.98, and 0.95, respectively.

Running the code, the familiar form of the output is displayed: (CAN WE ADD AN INTERPRETATION? )

```{r}
# metric_1 metric_2 metric_3
# 2.605283 1.090871 18.57181
```

This output mirrors that of the TE function shown earlier in that metric_2 is more reliable than metric_1, which is more reliable than metric_3. It differs from what the CV output indicates, however, which indicates that metric_2 is actually much less reliable than either metric_1 or metric_3. This pattern emerges because the CV is a unitless measure, whereas both the TE and the SEM derive their intepretations (at least in part) from the units in which they are measured. It is important to examine all of these statistics when making conclusions on the relative reliabilities of various metrics.   

## 2.4. Intraclass Correlation Coefficient: ICC_long()

The Intraclass Correlation Coefficient (ICC) is a measure of reliability that ranges from 0 to 1, with values closer to 1 indicating a greater degree of reliability. There are six different forms of the ICC that are appropriate for different experimental designs, as detailed in. The output of ICC_long() prints the ICC values of all six forms, for each metric that was entered as an argument to the function.

ICC_long() is simply a wrapper for psych::ICC(), but has two key differences: it can produce the ICC output for multiple metrics (as opposed to just one in psych::ICC()), and it takes data in long format as its inputs (as opposed to wide format in psych::ICC()), both of which are explained in detail below (Revelle, 2020).

To illustrate ICC_long() at work, the same example as above can be employed:

```{r}
psr::ICC_long(subject, trial, metric_1, metric_2, metric_3)
```

The function takes the same inputs as TE() and CV(), but yields output that takes a very different form:

```{r}
# $metric_1
# Call: psych::ICC(x = df)

# Intraclass correlation coefficients 
#                          type  ICC   F df1 df2       p lower bound upper bound
# Single_raters_absolute   ICC1 0.98 167   2   6 5.5e-06        0.91           1
# Single_random_raters     ICC2 0.98 167   2   4 1.4e-04        0.91           1
# Single_fixed_raters      ICC3 0.98 167   2   4 1.4e-04        0.88           1
# Average_raters_absolute ICC1k 0.99 167   2   6 5.5e-06        0.97           1
# Average_random_raters   ICC2k 0.99 167   2   4 1.4e-04        0.97           1
# Average_fixed_raters    ICC3k 0.99 167   2   4 1.4e-04        0.96           1

#  Number of subjects = 3     Number of Judges =  3
```

```{r}
# $metric_2
# Call: psych::ICC(x = df)

# Intraclass correlation coefficients 
#                          type  ICC  F df1 df2      p lower bound upper bound
# Single_raters_absolute   ICC1 0.82 14   2   6 0.0051        0.38        0.99
# Single_random_raters     ICC2 0.82 14   2   4 0.0147        0.38        0.99
# Single_fixed_raters      ICC3 0.82 14   2   4 0.0147        0.27        0.99
# Average_raters_absolute ICC1k 0.93 14   2   6 0.0051        0.64        1.00
# Average_random_raters   ICC2k 0.93 14   2   4 0.0147        0.64        1.00
# Average_fixed_raters    ICC3k 0.93 14   2   4 0.0147        0.52        1.00

#  Number of subjects = 3     Number of Judges =  3
```

```{r}
# $metric_3
# Call: psych::ICC(x = df)

# Intraclass correlation coefficients 
#                          type  ICC   F df1 df2       p lower bound upper bound
# Single_raters_absolute   ICC1 0.98 143   2   6 8.7e-06        0.90           1
# Single_random_raters     ICC2 0.98 143   2   4 1.9e-04        0.90           1
# Single_fixed_raters      ICC3 0.98 143   2   4 1.9e-04        0.87           1
# Average_raters_absolute ICC1k 0.99 143   2   6 8.7e-06        0.96           1
# Average_random_raters   ICC2k 0.99 143   2   4 1.9e-04        0.96           1
# Average_fixed_raters    ICC3k 0.99 143   2   4 1.9e-04        0.95           1

#  Number of subjects = 3     Number of Judges =  3
```

The output above is simply a list of the ICC's for each metric. Based on the ICC's, metric_1 and metric_3 each exhibit nearly perfect reliability, while metric_2 is noticeably less reliable, which is the same conclusion that can be drawn from the CV's of the metrics. The choice of which of the six ICC forms to use for the data entered is left to the user. Criteria for making this decision is discussed in Shrout & Fleiss (1979).

### 2.4.1. Different Argument Forms from psych::ICC()

The ICC function in the psych package takes an entire dataset as its only input. In addition, it also requires that the data be in "wide" form, so that each subject is its own row, the trials represent the columns, and the scores the subjects record fill in the entries of the data. A table in "wide" form looks like this:

| Subject | Trial 1 | Trial 2 | Trial 3 |
|--------:|:-------:|:-------:|:--------|
|    1    |   250   |   258   |   252   |
|    2    |   279   |   270   |   277   |
|    3    |   218   |   213   |   218   |

In reality many data are actually in "long" form, so that there are multiple rows for each subject and a column representing which trial each measurement came from. A data table in "long form" looks like this:

| Subject |  Trial  | Score |
|--------:|:-------:|:------|
|    1    | Trial 1 |  250  |
|    1    | Trial 2 |  258  |
|    1    | Trial 3 |  252  |
|    2    | Trial 1 |  279  |
|    2    | Trial 2 |  270  |
|    2    | Trial 3 |  277  |
|    3    | Trial 1 |  218  |
|    3    | Trial 2 |  213  |
|    3    | Trial 3 |  218  |

The ICC function in psr explicitly asks for the individual vectors corresponding to the subjects, trial, and measurements of each metric for which the user wants to see the ICC output, then puts each metric into its own "long" data frame with the subject and trial vectors, and finally converts this data frame into "wide" form (using tidyr::pivot_wider()), so that psych::ICC() can be run on each metric, displaying the same output that psych::ICC() would show (Wickham & Henry, 2020). In other words, our ICC function is a "wrapper" function for psych::ICC(), which makes the user's life much easier. This ensures that the underlying data from which the individual vectors come can be in long form, as long as the user can properly extract these vectors from the data.

### 2.4.2. Allowing More Metrics than psych::ICC()

An extension of the difference described in the previous section is the fact that the ICC function in psr allows for the simultaneous computation of the ICC for multiple metrics, however many the user wants, whereas psych::ICC() is limited in that, as its input data must be in wide form, it can only calculate the ICC for one metric at a time, due to the fact that data in "wide" form by construction include only the measurements of one metric.

To further explain the gained efficiency from using the new and improved version of the ICC function, if one wanted to see the ICC output of five different metrics from a given group of subjects and was using psych::ICC(), that person would need to create five different data frames, one for each metric, all in "wide" form, and pass each data frame to the function separately. To illustrate this point further, the user would need to run the following code for each metric of interest:

```{r, eval = FALSE}
tidyr::pivot_wider(data, names_from = "trial", values_from = "metric")
psych::ICC(data)
```

Conversely, using ICC_long(), the user must simply enter the subject and trial vectors before entering the five vectors containing the measurements for each metric, subsequently showing the ICC output for all five metrics after just one function call. In short, the user only needs to run the following code once:

```{r, eval = FALSE}
psr::ICC_long(subject, trial, metric_1, metric_2, metric_3, metric_4, metric_5)
```

This simplified process cuts down considerably on the time and effort required by the user in order to see the ICC output for all five metrics. In addition, all of the ICC outputs for the various metrics can be seen one directly after another, making it easier to compare the reliabilities of each metric. 

# 3. Setting Benchmarks for Athletes (What Constitutes Meaningful Change?)

The SWC() and MDC() functions allow practitioners to set benchmarks for athletes.

## 3.1. Smallest Worthwhile Change: SWC()

The Smallest Worthwhile Change (SWC) is a measure of the smallest change an athlete would need to exhibit from one trial (or set of trials) to another trial (or set of trials) in order for it to be considered worthwhile (i.e. practically meaningful), rather than merely the result of random or measurement error. It is computed by multiplying the between-athlete standard deviation by an effect size specified by the user (the default effect size is 0.2), as explained in Bernards et al. (2017).

For the SWC function, the user can choose which method should be used for the computation of the between-athlete standard deviation involved in the formula. Does the user want to average each subject's values and use the standard deviation of these averages (the default), or should only each subject's maximum or minimum value be considered? This is the purpose of the group_by and summarize functions from the dplyr package that are used inside of the SWC function.

Computing the SWC for the same data as in the prior three functions looks like this:

```{r}
psr::SWC(subject, trial, metric_1, metric_2, metric_3, effect_size = 0.2, method = 'AVG')
```

```{r}
# metric_1  metric_2 metric_3
# 5.963221 0.8666667 42.44559
```

Thus, in order for an athlete in the data to improve by a worthwhile amount, he or she would need to improve by 5.96 units for metric_1, 0.87 units for metric_2, and 42.45 units for metric_3, respectively.

Another application of the SWC is comparing it to the TE as a method of determining whether or not a set of measurements is reliable. If the TE is greater than the SWC for a metric, then the instrument is considered to be unreliable, as one could not be certain that an improvement equal to the SWC is worthwhile change, or rather is just error (Conway, 2017). For the data example in the preceding sections, the SWC is greater than the TE for both metric_1 and metric_3, while for metric_2, its TE is greater than its SWC. This indicates that the measurement instrument being used in this data is reliable for measuring metric_1 and metric_3, but unreliable for measuring metric_2.

## 3.2. Minimal Detectable Change: MDC()

With the same goal in mind as the SWC of capturing individual change, the Minimal Detectable Change (MDC) is simply the SEM  multiplied by root 2 and then multiplied by a critical value that corresponds to a confidence level that the user chooses. It is described in Riemann & Lininger (2018). It also appears in Beekhuizen et al. (2009), de Vet et al. (2006).  

This function is similar to SWC(), but has some important differences. The main such difference is that it includes the ICC in its calculation, and it is based on a critical value of the normal distribution. This critical value is based on how confident the user wants to be that the MDC value generated cannot be explained solely by chance or error. The default confidence level is 0.95, meaning that, if the MDC indicated by the function were to be achieved by an athlete in the data, one could 95% confident is a legitimate change in performance, based on the corresponding critical value from the standard normal distribution. In other words, such a change would be statistically significant at the 5% level.

Using this formula with the usual data, the resulting output is:

```{r}
psr::MDC(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.99, 0.93, 0.99), confidence = 0.95)
```

```{r}
# metric_1 metric_2 metric_3
# 7.221344 3.023685 51.47747
```

There is one important thing to note from this output. The MDC is larger than the SWC for each metric, which sheds light on the fact that the MDC represents a high threshold for improvement. If an athlete were to achieve a change equal to the MDC, with the default confidence level of 95%, one could be 95% confident that the change was not the result of error. One might not need to have that degree of certainty, and might prefer to set a benchmark for their athletes that is more achievable. Changing the confidence level from 95% to 80% lowers the MDC of each metric considerably, as shown in the code and output below: 

```{r}
psr::MDC(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.99, 0.93, 0.99), confidence = 0.80)
```

```{r}
# metric_1 metric_2 metric_3
# 4.721783 1.977081 33.65931
```

These MDC's represent changes that, if achieved, a practitioner could be 80% confident is an actual improvement, rather than being due to random chance alone. The important point to remember is that as one decreases the confidence level in MDC(), the MDC also decreases and might be easier for athletes to achieve, but the level of certainty one can have that a change equal to the MDC is a real change decreases as well.

# 4. Standard Ten Scores: STEN() 

The previous functions have focused on setting benchmarks for athletes and measuring the reliability of the instruments involved in the data collection, and they have represented group-wide concepts. Practitioners might also wish to conduct analysis at the individual level. To achieve this purpose, psr includes the STEN() function. 

Practitioners may wish to analyze how each athlete compares to his or her teammates or competitors in the same data. For this purpose, STEN() is part of psr, and it represents the Standard Ten (STEN) scores of the athletes for each metric (Coaley, 2010). The STEN scores range from one to ten, with a mean of 5.5. The function has the simplest form of any function in the package (similar to TE() and CV()), as it takes just the subject, trial, and metric vectors as inputs:

```{r}
psr::STEN(subject, trial, metric_1, metric_2, metric_3)
```

It produces simple output that looks as follows for the same data that was used to demonstrate all of the previous functions:

```{r}
# subject   trial metric_1 metric_2 metric_3
#       1 Trial 1     5.63     4.85     5.07
#       1 Trial 2     6.24     3.40     5.74
#       1 Trial 3     5.78     4.85     5.88
#       2 Trial 1     7.85     6.79     3.17
#       2 Trial 2     7.16     8.73     2.87
#       2 Trial 3     7.70     8.25     3.51
#       3 Trial 1     3.17     5.34     7.95
#       3 Trial 2     2.79     3.40     7.77
#       3 Trial 3     3.17     3.88     7.54
```

This output lends insight into how well each athlete performed on each trial, compared to all of the measurements recorded in the data for the same metric. One can see that subject 1 recorded close to the mean of each metric for most of his or her measurements. In Trials 1 and 2, subject 2 recorded high measurements for all three metrics but low measurements in Trial 3, while the opposite was the case for subject 3. This function makes it easy to see how each athlete is performing relative to the other athletes in the data for each of his or her measurements, and how this performance differs across trials and metrics, respectively.

# 5. Error Examples

The last section of this vignette explains potential errors that could occur in entering data into the functions in psr, why they would be problematic, and shows the informative error messages that are produced when the user makes these mistakes. TE() is used to illustrate each potential error, but entering the same data into any of the functions will produce the same error message, as a separate function to catch specific types of errors has been embedded within all of the functions in this package.

# 5.1. Trials Not Labeled Uniformly Across Athletes

One potential error that could be made is the trials not being labeled in the same way across each athlete. To see an example of this, consider the following code:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 4')
# metric_1 <- c(250, 258, 252, 279, 270, 277, 218, 213, 218)
# metric_2 <- c(10, 7, 10, 14, 18, 17, 11, 7, 8)
# metric_3 <- c(1214, 1276, 1289, 1037, 1010, 1069, 1481, 1465, 1443)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

The problem is that subject 3's third measurement is labeled as Trial 4, rather than Trial 3, as is the case for the other two subjects. Subject 3 does not record a measurement for Trial 3, while neither Subject 1 nor Subject 2 record a measurement for Trial 4. If one were to run the following code, the following error message would show up:

```{r}
# [1] "Each athlete must have recorded a measurement for each trial"
```

This message precisely explains the issue, which is that each athlete has not recorded a measurement for each trial. This message precedes the unhelpful R message that is produced when any function does not work properly, meaning that it is visible to the user directly under the code that had just been entered.

# 5.2. Duplicate Trials for an Athlete

What if an athlete has recorded multiple measurements for one trial? An example of this is shown below:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 2')
# metric_1 <- c(250, 258, 252, 279, 270, 277, 218, 213, 218)
# metric_2 <- c(10, 7, 10, 14, 18, 17, 11, 7, 8)
# metric_3 <- c(1214, 1276, 1289, 1037, 1010, 1069, 1481, 1465, 1443)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

As one can see, Subject 3 has recorded two measurements for Trial 2. One would be correct to note that this subject has also not recorded a measurement for Trial 3, hence Error 1 has also occurred in this code. If both errors occur, only the message for Error 2 is shown to the user, as it does in this example:

```{r}
# [1] "Each athlete must not have recorded multiple measurements for any trial"
```

This message is shown and offers a clear and concise indicator of what has gone wrong in the code that the user had just entered.

# 5.3. Character Vector for Metric

In the following error example, the trials are all labeled correctly, but the first metric is entered as a character vector, when it needs to be a numeric vector:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3')
# metric_1 <- c("abc", "def", "ghi", "jkl", "mno", "pqr", "stu", "vwx", "yzz")
# metric_2 <- c(10, 7, 10, 14, 18, 17, 11, 7, 8)
# metric_3 <- c(1214, 1276, 1289, 1037, 1010, 1069, 1481, 1465, 1443)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

When this code is run, the following helpful error message is displayed, directly below the code that the user had just entered:

```{r}
# [1] "Each metric must be numeric"
```

Yet again, the user knows exactly why the code did not run properly.

# 6. References

Atkinson, G., & Nevill, A. M. (1998). Statistical Methods For Assessing Measurement Error (Reliability) in Variables.

Beekhuizen, K., Davis, M. D., Kolber, M. J., Cheng, M. (2009). Test-Retest Reliability and Minimal Detectable Change of the Hexagon Agility Test. The Journal of Strength and Conditioning Research, 23(7), 2167-2171.

Bernards, J., Sato, K., Haff, G., & Bazyler, C. (2017). Current Research and Statistical Practices in Sport Science and a Need for Change. Sports, 5(4), 87.

Coaley, K. (2010). Statistics for psychological measurement. In An introduction to psychological assessment and psychometrics (pp. 72-96). SAGE Publications Ltd, https://www.doi.org/10.4135/9781446221556.n4

Conway, B. (2017). Smallest Worthwhile Change. https://www.scienceforsport.com/smallest-worthwhile-change/

de Vet, H. C., Terwee, C. B., Ostelo, R.W., Beckerman, H., Knol D. L., Bouter, L. M. (2006). Minimal changes in health status questionnaires: distinction between minimally detectable change and minimally important change. Health and Quality of Life Outcomes, 4(54). 

Ferger, K., & Büsch, D. (2018). Individual measurement of performance change in sports. Deutsche Zeitschrift Für Sportmedizin, 2018(02), 45-52.

Hopkins, W. G. (2000). A new view of statistics. Internet Society for Sport Science: http://www.sportsci.org/resource/stats/.

Hopkins, W. G. (2000). Measures of Reliability in Sports Medicine and Science. Sports Medicine, 30(5), 375-381.

Revelle W (2020). psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern University, Evanston, Illinois. R package version 2.0.12, https://CRAN.R-project.org/package=psych.

Riemann, B. L., &amp; Lininger, M. R. (2018). Statistical Primer for Athletic Trainers: The Essentials of Understanding Measures of Reliability and Minimal Important Change. Journal of Athletic Training, 53(1), 98-103.

Shrout, P. E., &amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420-428.

Wickham, H., & Henry, L. (2020). tidyr: Tidy Messy Data. R package version 1.1.0. https://CRAN.R-project.org/package=tidyr
