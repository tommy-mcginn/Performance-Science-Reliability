title: "psr Explained"
author: "Tommy McGinn"
date: "2/8/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{psr Explained}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# 1. Motivation for Package

The psr package is intended for coaches and analysts in the field of sports performance science. It aims to allow users to efficiently calculate metrics for assess measurement reliability as well as individual change in athletic performance. All functions take as its inputs vectors containing measurements taken on an ath (WE NEED TO TALK ABOUT THIS WORDING WITH MATT)

psr can be used to assess measurement reliability via functions that compute the typical error (TE), coefficient of variation (CV), standard error of measurement (SEM), and intraclass correlation coefficient (ICC_long) functions. ICC_long() builds on psych::ICC() by taking data in long format as its input, saving the user the task of pivoting the data to wide format. Furthermore, ICC_long allows for the computation of ICC's for multiple metrics at once.

The smallest worthwhile change (SWC) and minimal detectable change (MDC) functions assess individual change of athletes, but are still group-wide metrics.  Individual-level metrics also appear in the package, in the form of the standard ten score (STEN) and probability of reliable change (PROBRC) functions. STEN() takes each athlete's scores for every metric and standardizes them to a 1-10 scale, allowing the user to easily see an athlete's strengths and weaknesses relative to his or her peers, while PROBRC() computes the probability that the change made by each athlete from one trial to another was reliable. This vignette illustrates the use and intreprets the output of each function via example data.     

# 2. Reliability of Measurement Instrument

The psr package includes four different functions for assessing reliability of measurements: TE(), CV(), SEM(), and ICC_long(). Each will be discussed in turn.

```{r}
library(psr)
```

## 2.1. Typical Error: TE()

The Typical Error (TE) is the within-athlete variation of the athlete measurements in the data. It is a measure of how much one should "typically" expect the measurements of each athlete to vary from trial to trial, due to noise in the data. Such noise could be the result of biological variation in the athletes or measurement error on the part of the instrument being used to record the in the test that the athletes are completing. If there are just two trials being analyzed, the TE is just the standard deviation of the difference scores of each athlete between the first trial and the second trial, divided by root 2. (Hopkins, 2000)

If more than two trials are being studied, the TE is the residual standard error of a linear model with the metric as the response and the subject and trial as independent variable, entered into the model as factors (Hopkins,2000). It represents the error involved in predicting the value of a metric based on who is being measured and which trial that athlete is on (LET'S WORK ON THIS SENTENCE).  

The following data set consists of three different athletes performing three trials of a certain task, and three different metrics on biomechanical measurements are recorded:

```{r}
subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3')
metric_1 <- c(257, 268, 237, 275, 259, 263, 216, 287, 250)
metric_2 <- c(1.11, 1.24, 0.89, 1.37, 1.21, 1.30, 0.75, 1.42, 1.15)
metric_3 <- c(1272, 1493, 1072, 1046, 1198, 1165, 1478, 1370, 1335)
```

The TE() function is called by specifying the rows containing the subjects, trials, and metrics:

```{r}
TE(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# metric_1  metric_2 metric_3
# 23.53248 0.2369951 134.7632
```

The TE for metric_3 is much larger than the TE for metric_2, but that is largely due to the fact that metric_3 is on a different scale than metric_2. Thus, the TE is expressed in terms of the units of the original metric.

## 2.2. Coefficient of Variation: CV()

The Coefficient of Variation (CV) is the TE expressed as a percentage of the mean of the data. It is calculated by first computing the TE of each metric, and dividing this value by the mean of the metric, across all athletes and trials. There are inconsistent explanations of the CV in the performance science literature, and this package follows the interpretation found in Hopkins (2000), which denotes the CV as the typical percentage error. Thus, the TE is expressed in absolute terms, while the CV is expressed in percentage terms. The CV is a unitless measure, which allows for easy comparison of metrics with different units and scales of measurement, whereas the interpretation of the TE is entirely dependent on unit and scale.

```{r}
CV(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# metric_1 metric_2 metric_3
# 9.160569 20.43061  10.6122
```

Using the same data as in the section above, it is seen that the CV of metric_2 is actually almost double the CV of metric_3. Hopkins notes that the CV for a test will typically be 1-5%. In comparison, the TE of metric_2 was much smaller than the TE of metric_3 due to the relative magnitudes of the metrics. 

## 2.3. Standard Error of Measurement: SEM()

The Standard Error of Measurement (SEM) is similar to the TE and CV, and is a way of quantifying the amount of variability between athletes. There is another difference between this function and the other two previously mentioned: it utilizes the ICC, which is another measure of reliability. The formula for the SEM is the between-subject standard deviation of each athlete's scores multiplied by root (1 - ICC), as described in Atkinson & Nevill (1998).

![Alt text] (/Users/tmcginn/Desktop/Unknown-1.png)

Running the code for SEM(), using the same data as in the case of the TE() and CV():

```{r}
SEM(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.92, 0.98, 0.95))
```

As noted earlier, SEM() adds one additional argument to those in TE() and CV(). This addition is a list of the individual ICC values for the metrics, which must be entered in order. In the above example, the ICC values for metric_1, metric_2, and metric_3 are 0.92, 0.98, and 0.95, respectively.

Running the code, the familiar form of the output is displayed: (CAN WE ADD AN INTERPRETATION?)

```{r}
# Metric metric_1  metric_2 metric_3
#    SEM  5.92865 0.0309758  36.4701
```

## 2.4. Intraclass Correlation Coefficient: ICC_long()

The Intraclass Correlation Coefficient (ICC) is a measure of reliability that ranges from 0 to 1, with values closer to 1 indicating a greater degree of reliability. There are six different forms of the ICC that are appropriate for different experimental designs, as detailed in. The output of ICC_long() prints the ICC values of all six forms, for each metric that was entered as an argument to the function.

ICC_long() is simply a wrapper for psych::ICC(), but has two key differences: it can produce the ICC output for multiple metrics (as opposed to just one in psych::ICC()), and it takes data in long format as its inputs (as opposed to wide format in psych::ICC()), both of which are explained in detail below. (NEED TO CITE psych PACKAGE)

To illustrate ICC_long() at work, the same example as above can be employed:

```{r}
ICC_long(subject, trial, metric_1, metric_2, metric_3)
```

The function takes the same inputs as TE() and CV(), but yields output that takes a very different form:

```{r}

```

The output above is simply a list of the ICC's for each metric. ... The choice of which ICC form to use for the data entered is left to the user.

### 2.4.1. Different Argument Forms from psych::ICC()

The ICC function in the psych package takes entire data as its only input. In addition, it also requires that the data be in the "wide" form, so that each subject is its own row, the trials represent the columns, and each subject's measurement from each trial fills in the entries of the data. A table in this "wide" form of data looks like this:

| Subject | Trial 1 | Trial 2 | Trial 3 |
|--------:|:-------:|:-------:|:--------|
|    1    |   275   |   208   |   239   |
|    2    |   217   |   260   |   274   |
|    3    |   267   |   261   |   253   |

In reality many data are actually in "long" form, so that there are multiple rows for each subject and a column representing which trial each measurement came from. A data table in "long form" looks like this:

| Subject |  Trial  | Score |
|--------:|:-------:|:------|
|    1    | Trial 1 |  275  |
|    1    | Trial 2 |  208  |
|    1    | Trial 3 |  239  |
|    2    | Trial 1 |  217  |
|    2    | Trial 2 |  260  |
|    2    | Trial 3 |  274  |
|    3    | Trial 1 |  267  |
|    3    | Trial 2 |  261  |
|    3    | Trial 3 |  253  |

The ICC function in psr (WHAT ARE WE CALLING THE FUNCTION? psr OR PerfSciReliability?) explicitly asks for the individual vectors corresponding to the subjects, trial, and measurements of each metric for which the user wants to see the ICC output, then puts each metric into its own "long" data frame with the subject and trial vectors, and finally converts this data frame into "wide" format (using the pivot_wider function in the tidyr package), so that the ICC function from the psych package can be run on each metric, displaying the same output that the ICC function from the psych package would display. In other words, our ICC function is a "wrapper" function for that in the psych package, which makes the user's life much easier. This ensures that the underlying data from which the individual vectors come can be in whatever format, as long as the user can properly extract these vectors from the data.

### 2.4.2. Allowing More Metrics than psych::ICC()

An extension of the difference described in the previous section is the fact that the ICC function in the PerfSciReliability package allows for the simultaneous computation of the ICC for multiple metrics, however many the user wants, whereas the ICC function in the psych package is limited in that, as it takes as its input data in "wide" format, it can only calculate the ICC for one metric at a time, due to the fact that data in "wide" format by construction include only the measurements of one metric.

To further explain the gained efficiency from using the new and improved version of the ICC function, if one wanted to see the ICC output of five different metrics from a given group of subjects and was using the ICC function in the psych package, that person would need to create five different data frames, one for each metric, all in "wide" form, and pass each data frame to the ICC function separately. To illustrate this point further, the user would need to run the following code for each metric of interest:

```{r, eval = FALSE}
tidyr::pivot_wider(data, names_from = "trial", values_from = "metric")
psych::ICC(data)
```

Conversely, using ICC_long(), the user could simply enter the subject and trial vectors before entering the five vectors containing the measurements for each metric, subsequently showing the ICC output for all five metrics after just one function call. In short, the user only needs to run the following code once:

```{r, eval = FALSE}
ICC_long(subject, trial, metric1, metric2, metric3, metric4, metric5)
```

This simplified process cuts down considerably on the time and effort required by the user in order to see the ICC output for all five metrics. In addition, all of the ICC outputs for the various metrics can be seen one directly after another, making it easier to compare the reliability of each metric. 

# 3. Setting Benchmarks for Athletes (What Constitutes Meaningful Change?)

The SWC() and MDC() function allow practitioners to set benchmarks for athletes.

## 3.1. Smallest Worthwhile Change: SWC()

The Smallest Worthwhile Change (SWC) is a measure of the smallest change an athlete would need to exhibit from one trial (or set of trials) to another trial (or set of trials) in order for it to be considered worthwhile (i.e. practically meaningful), rather than merely the result of random or measurement error. It is computed by multiplying the between-athlete standard deviation by an effect size specified by the user (the default effect size is 0.2).

For the SWC function, the user can choose which method should be used for the computation of the between-athlete standard deviation involved in the formula. Does the user want to average each subject's values and use the standard deviation of these averages (the default), or should only each subject's maximum or minimum value be considered? This is the purpose of the group_by and summarise functions from the dplyr package that are used inside of the SWC function:

Computing the SWC for the same data as in the prior three functions looks like this:

```{r}
SWC(subject, trial, metric_1, metric_2, metric_3, effect_size = 0.2, method = 'AVG')
```

```{r}
# Metric metric_1   metric_2 metric_3
#    SWC 4.192189 0.04380639 32.61985
```

Thus, in order for an athlete in the data to improve by a worthwhile amount, he or she would need to improve by 4.19 units for metric_1, 0.04 units for metric_2, and 32.62 units for metric_3, respectively.

Another application of the SWC is comparing it to the TE as a method of determining whether or not a set of measurements is reliable. If the TE is greater than the SWC for a metric, then the instrument is considered to be unreliable, as one could not be certain that an improvement equal to the SWC is worthwhile change, or rather is just error. (CITATION?) 
For the data example in the preceding sections, the TE is much greater than the SWC for every metric. This indicates that the measurement instrument being used in this data may not reliable for any of the three metrics.

## 3.2. Minimal Detectable Change: MDC()

With the same goal in mind as the SWC of capturing individual change, the Minimal Detectable Change (MDC) is simply the SEM  multiplied by root 2 and then multiplied by a critical value that corresponds to a confidence level that the user chooses. This function is similar to SWC(), but has some important differences. The main such difference is that it includes the ICC in its calculation, and it is based on a critical value of the normal distribution. This critical value is based on how confident the user wants to be that the MDC value generated cannot be explained solely by error. The default confidence level is 0.95, meaning that, if the MDC indicated by the function were to be achieved by an athlete in the data, one could 95% confident is a legitimate change in performance, based on the corresponding critical value from the standard normal distribution. In other words, such a change would be statistically significant at the 5% level. The formula for the MDC is:

Using this formula with the usual data, the resulting output is:

```{r}
MDC(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.92, 0.98, 0.95), confidence = 0.95, method = 'AVG')
```

```{r}
# Metric metric_1   metric_2 metric_3
#    MDC 16.43308 0.08585895 101.0881
```

There is one important thing to note from this output. The MDC is much larger than the SWC for each metric, which could be due to the fact that the measurement instrument is unreliable based on a comparison of the TE and the SWC. This confirms that a value larger than the SWC is needed for a change to be significant, which is provided by the MDC. The MDC is still smaller than the TE for all three metrics, however, which means that a change could be considered statistically significant but still the result of error, if the measurement instrument is extremely unreliable.

# 4. Individual Level Metrics

## 4.1. Standard Ten Scores: STEN()

The previous functions have focused on setting benchmarks for athletes and measuring the reliability of the instruments involved in the data collection.In addition, practitioners may wish to analyze how each athlete compares to his or her teammates or competitors in the same data. For this purpose, STEN() is part of psr, and it represents the Standard Ten (STEN; CITATION) scores of the athletes for each metric. The STEN scores range from one to ten, with a mean of 5.5. The function has the simplest form of any function in the package (similar to TE() and CV()), as it takes just the subject, trial, and metric vectors as inputs:

```{r}
STEN(subject, trial, metric_1, metric_2, metric_3)
```

It produces simple output that looks as follows for the same data that was used to demonstrate all of the previous functions:

```{r}
# subject   trial metric_1 metric_2 metric_3
#       1 Trial 1     5.51     5.04     5.53
#       1 Trial 2     6.56     6.23     8.24
#       1 Trial 3     3.60     3.03     3.07
#       2 Trial 1     7.23     7.42     2.75
#       2 Trial 2     5.70     5.96     4.62
#       2 Trial 3     6.08     6.78     4.21
#       3 Trial 1     1.60     1.76     8.05
#       3 Trial 2     8.37     7.87     6.73
#       3 Trial 3     4.84     5.41     6.30
```

This output lends insight into how well each athlete performed on each trial, compared to all of the measurements recorded in the data for the same metric. One can see that subject 1 recorded mostly higher than average measurements for Trial 1 and Trial 2, but these measurements were lower than the average for all three metrics in Trial 3. In Trial 1, subject 2 recorded high measurements for metric 1 and metric 2 and a low measurement for metric 3 compared to his or her peers, while the opposite was the case for subject 3. This function makes it easy to see how each athlete is performing relative to the other athletes for a given trial and metric combination, and how this performance differs across trials and metrics, respectively.

# 4.2. Probability of Reliable Change: PROBRC()

Similar to STEN(), PROBRC() calculates an individual-level metric, as opposed to the other functions in the package described in Sections 2 and 3, all of which yield one output statistic for the entire group represented in the data. PROBRC() computes the probability that a given change from one trial to another was reliable, for each athlete in the data. As in the other functions, this function takes the subject, trial, and metric vectors as inputs, in addition to the ICC's of each of the metrics. The user must make an additional specification when calling the function: he or she must indicate which trials in the data should represent the initial and final trials, respectively, that will be compared in the calculation of the probability of reliable change.

The following example takes the same data as above:

```{r}
PROBRC(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.92, 0.95, 0.98), initial = which(trial == "Trial 1"), final = which(trial == "Trial 2"))
```

As one can see, the user wants to see whether "Trial 2" was a reliable change from "Trial 1", for each athlete and metric. It yields the following output:

```{r}
# subject metric_1 metric_2 metric_3
#       1        1        1        1
#       1        1        1        1
#       1        1        1        1
#       2        1        1        1
#       2        1        1        1
#       2        1        1        1
#       3        1        1        1
#       3        1        1        1
#       3        1        1        1
```

The probability calculations shown in the table are all rounded to two decimal places, meaning that any probability of greater than or equal to 0.995 will be appear as a probability of 1, as is shown in the output above. In effect, for each athlete's "Trial 2" measurements for each of the three metrics, one can be 100% certain that he or she has exhibited a reliable change from his or her  measurement in "Trial 1".

# 5. Error Examples

This vignette explains potential errors that could occur in entering data into the functions in psr, why they would be problematic, and shows the informative error messages that are produced when the user makes these mistakes when running the functions. TE() is used to illustrate each potential error, but entering the same data into any of the functions should produce the same error message.

# 5.1. Trials Not Labeled Uniformly Across Athletes

One potential error that could be made is the trials not being labeled in the same way across each athlete. To see an example of this, consider the following code:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 4')
# metric_1 <- c(257, 268, 237, 275, 259, 263, 216, 287, 250)
# metric_2 <- c(1.11, 1.24, 0.89, 1.37, 1.21, 1.30, 0.75, 1.42, 1.15)
# metric_3 <- c(1272, 1493, 1072, 1046, 1198, 1165, 1478, 1370, 1335)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

The problem is that subject 3's third measurement is labeled as Trial 4, rather than Trial 3, as is the case for the other two subjects. Subject 3 does not record a measurement for Trial 3, while neither Subject 1 nor Subject 2 record a measurement for Trial 4. If one were to run the following code, the following error message would show up:

```{r}
# [1] "Each athlete must have recorded a measurement for each trial"
```

This message precisely explains the issue, which is that each athlete has not recorded a measurement for each trial. This message precedes the unhelpful R message that is produced when any function does not work properly, meaning that it is visible to the user directly under the code that had just been entered.

# 5.2. Duplicate Trials for an Athlete

What if an athlete has recorded multiple measurements for one trial? An example of this is shown below:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 2')
# metric_1 <- c(257, 268, 237, 275, 259, 263, 216, 287, 250)
# metric_2 <- c(1.11, 1.24, 0.89, 1.37, 1.21, 1.30, 0.75, 1.42, 1.15)
# metric_3 <- c(1272, 1493, 1072, 1046, 1198, 1165, 1478, 1370, 1335)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

As one can see, Subject 3 has recorded two measurements for Trial 2. One would be correct to note that this subject has also not recorded a measurement for Trial 3, hence Error 1 has also occurred in this code. If both errors occur, only the message for Error 2 is shown to the user, as it does in this example:

```{r}
# [1] "Each athlete must not have recorded multiple measurements for any trial"
```

This message is shown and offers a clear and concise indicator of what has gone wrong in the code that the user had just entered.

# 5.3. Character Vector for Metric

In the following error example, the trials are all labeled correctly, but the first metric is entered as a character vector, when it needs to be a numeric vector:

```{r}
# subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
# trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3')
# metric_1 <- c("abc", "def", "ghi", "jkl", "mno", "pqr", "stu", "vwx", "yzz")
# metric_2 <- c(1.11, 1.24, 0.89, 1.37, 1.21, 1.30, 0.75, 1.42, 1.15)
# metric_3 <- c(1272, 1493, 1072, 1046, 1198, 1165, 1478, 1370, 1335)
# TE(subject, trial, metric_1, metric_2, metric_3)
```

When this code is run, the following helpful error message is displayed, directly below the code that the user had just entered:

```{r}
# [1] "Each metric must be numeric"
```

Yet again, the user knows exactly why the code that he or she had just entered did not run properly.

# 6. References

Atkinson, G., & Nevill, A. M. (1998). Statistical Methods For Assessing Measurement Error (Reliability) in Variables.

Hopkins, W. G. (2000). Measures of Reliability in Sports Medicine and Science. Sports Medicine, 30(5), 375-381.

Revelle W (2020). psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern University, Evanston, Illinois. R package version 2.0.12, https://CRAN.R-project.org/package=psych.

https://www.sportsci.org/resource/stats/precision.html
