---
title: "psr Explained"
author: "Tommy McGinn"
date: "12/23/2020"
output: html_document
---

# Motivation for Package

This package is intended for coaches and analysts in the field of performance science. It is meant to allow its user to gain insight into metrics of both measurement reliability as well as individual change, given data from a set of athletes. The functions in psr are all built around different themes. One of these themes are measures of the standard error between athletes, which is captured in the typical error (TE), coefficient of variation (CV), and standard error of measurement (SEM) functions. Another theme pertains to determining the smallest change from the baseline needed to be considered meaningful. That is where the smallest worthwhile change (SWC) and minimal detectable change (MDC) enter the picture. One can also use the MDC to set specific benchmarks for athletes for future measurements, which is what the reliable change index (RCI) function accomplishes. In addition, the intraclass correlation coefficient (ICC) function builds on the ICC function in psych by taking data in long format as its input, saving the task of pivoting the data to wide format. Finally, the STEN function takes each athlete's scores for every metric and standardizes them to a 0-10 scale, allowing the user to easily see an athlete's strengths and weaknesses relative to his or her peers.      

```{r cars}
summary(cars)
```

# Standard Error Within Athletes

There are three different ways which psr offers to calculate the standard error between athletes: TE, CV, and SWC. 

## TE()

The typical error is the within-athlete standard deviation of the data, and it is the squared residual of a linear model with the metric measurement as the response and the subject and trial variables (both as factors) as the predictors. It represents the error involved in predicting the value of a metric based on who is being measured and which trial that athlete is on.

As an example, one can use simple data of three different athletes performing three trials each:

```{r}
subject <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)
trial <- c('Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3', 'Trial 1', 'Trial 2', 'Trial 3')
metric_1 <- c(257, 268, 237, 275, 259, 263, 216, 287, 250)
metric_2 <- c(1.11, 1.24, 0.89, 1.37, 1.21, 1.30, 0.75, 1.42, 1.15)
metric_3 <- c(1272, 1493, 1072, 1046, 1198, 1165, 1478, 1370, 1335)
TE(subject, trial, metric_1, metric_2, metric_3)
```

Running the code, this produces the following output:

```{r}
TE(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# Metric metric_1  metric_2 metric_3
#     TE 23.53248 0.2369951 134.7632
```

The output is a neat table that reports the TE values of each of the three metrics. One issue that comes up, however, is that of interpreting these values. The TE for metric_3 is much larger than the TE for metric_2, but that is largely due to the fact that metric_3 is on a different scale than metric_2. How can one tell if the TE is high, low, or just right? The CV gives an answer to that question.

## CV()

The coefficient of variation is similar to the TE, but it scales all sets of measurements to the same range, no matter the scale of the different measurements. It does this by first taking the within-athlete standard deviation of each measurement, and dividing this value by each athlete's mean measurement value. This leaves one CV for each athlete, for each metric. Finally, these CV's are averaged for each metric, yielding the CV for the metric that is reported to the user.

Using the same data as in the TE() section above and running the code, one gets:

```{r}
CV(subject, trial, metric_1, metric_2, metric_3)
```

```{r}
# Metric metric_1 metric_2 metric_3
#     CV 7.823127 17.68113 9.615649
```

As one can see, the CV function yields a way of comparing across metrics. Whereas the TE of metric_2 was much smaller than the TE of metric_3 based solely on the relative magnitudes of the metrics, the CV of metric_2 is actually almost double the CV of metric_3. In other words, metric_2 is much less reliable than metric_3. Generally speaking, a CV of greater than 10 is considered to be reliable. This means that metric_2 is highly unreliable, while metric_3 is marginally unreliable and metric_1 is solidly reliable.

## SEM()

The SEM is similar to the CV, but has one key difference: it utilizes the ICC, which is another measure of reliability. It is calculated by using the following formula:

[Insert image of formula here]

Running the code for SEM(), using the same data as in the case of the TE() and CV():

```{r}
SEM(subject, trial, metric_1, metric_2, metric_3, ICC = c(0.92, 0.98, 0.95), method = 'AVG')
```

SEM() adds two additional arguments to those in TE() and CV(). The first is a list of the individual ICC values for the metrics (which must be entered in order), while the second is a way of indicating whether the user desires the baseline standard deviation of the athletes should be computed using the average, maximum, or minimum value of each athlete. The most common way is by using the average value of each athlete as above ('AVG'), but one can also use the maximum ('MAX') or minimum ('MIN') values.

Running the code, the familiar form of the output is displayed:

```{r}
# Metric metric_1  metric_2 metric_3
#    SEM  5.92865 0.0309758  36.4701
```

# Reliability of Measurement Instrument

There are a couple of functions to measure the reliability of the measurement instrument from which the data comes. The main function that achieves this purpose is ICC(). This function is similar to psych::ICC(), but has two key differences: it can produce the ICC output for multiple metrics (as opposed to just one in psych::ICC()), and it takes data in long format as its inputs (as opposed to wide format in psych::ICC()).

The ICC is a measure of reliability that ranges from 0 to 1, with values closer to 1 indicating a greater degree of reliability. There are six different forms of the ICC that are appropriate for different circumstances, as detailed in. The output of ICC() prints the ICC values of all six forms.

To illustrate ICC() at work, the same example as above can be employed:

```{r}
ICC(subject, trial, metric_1, metric_2, metric_3)
```

The function takes the same inputs as TE() and CV(), but yields output that takes a very different form:

```{r}

```

The output above is simply a list of the ICC's for each metric. ... The choice of which ICC form to use for the data entered is left to the user.

# Setting Benchmarks for Athletes (What Constitutes Reliable Change?)

There are a couple of functions that allow practitioners to set benchmarks for athletes: the SWC and the MDC. The RCI is an extension of the MDC and also appears in our package.

## SWC()

The SWC is a measure of the smallest change an athlete would need to exhibit from one trial (or set of trials) to another trial (or set of trials) in order for it to be considered worthwhile. It is computed by multiplying the between-athlete standard deviation by an effect size specified by the user (the default effect size is 0.2):

[Insert image of formula here]

The larger the effect size the user desires, the larger the value of the SWC will be. Another source of user input is the values to be used when computing the between-athlete standard deviation. This method argument takes either 'MIN', 'MAX', or 'AVG', as described in the SEM() section earlier in this vignette. Using the same data as previously, the output looks as follows:

```{r}
SWC(subject, trial, metric_1, metric_2, metric_3, effect_size = 0.2, method = 'AVG')
```

```{r}
# Metric metric_1   metric_2 metric_3
#    SWC 4.192189 0.04380639 32.61985
```

The interpretation for this output is as follows: in order for an athlete in the data to improve by a worthwhile amount, he or she would need to improve by 4.19 units for metric_1, 0.04 units for metric_2, and 32.62 units for metric_3, respectively.

## Comparing the TE to the SWC

Another way of determining whether or not a measurement instrument is reliable is by comparing its TE values to its SWC values for each metric. If the TE is greater than the SWC for a metric, then the instrument is considered to be unreliable, as one could not be certain that an improvement equal to the SWC is worthwhile change, or rather is just error. Unsurprisingly, a practitioner should desire a TE that is less than the SWC for every metric.

For the data utilized in the preceding sections, the TE is much greater than the SWC for every metric. This is a red flag that the measurement instrument being used in this data is not reliable for any of the three metrics, so one cannot be confident that the SWC represents a worthwhile change and not just error.

## MDC()

Another way of quantifying change that is larger than the size of error in the measurement instrument is the MDC. This function is similar to SWC(), but has some important differences. The main such difference is that it includes the ICC in its calculation, and it is based on a critical value of the normal distribution. This critical value is in turn based on how confident the user wants to be that the MDC value generated cannot be explained solely by error. It is essentially a manifestation of how large of a change would need to occur in order for it to be statistically significant. The formula for the MDC is:

[Insert image of formula here]

Using this formula with the usual data, the resulting output is:

```{r}
MDC(subject, trial, metric_1, metric_2, metric_3, ICC, confidence = 0.95, method = 'AVG')
```

```{r}
# Metric metric_1   metric_2 metric_3
#    MDC 16.43308 0.08585895 101.0881
```

There is one important thing to note from this output. The MDC is much larger than the SWC for each metric, which could be due to the fact that the measurement instrument is unreliable based on a comparison of the TE and the SWC. This confirms that a value larger than the SWC is needed for a change to be significant, which is provided by the MDC. The MDC is still smaller than the TE for all three metrics, however, which means that a change could be considered statistically significant but still the result of error, if the measurement instrument is extremely unreliable.

## An Application of the MDC: RCI()

# Standardizing Scores Across Athletes
